{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d15ca570-4e81-4bc8-8c25-8af67a745adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55c2c9a1-46ab-4a80-a142-b1a733d0451d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Getting Started with MLflow and Mosaic AI Model Serving\n",
    "\n",
    "In this lesson, we will take a second look at MLflow outside the perspective of AutoML. To this end, this notebook uses a random forest classifier as an example model so we can demonstrate how to log a model. We will demonstrate how to use Mosaic AI Model Serving for real-time inferencing. Finally, we give a brief overview of how to utilize Workflows for machine learning. \n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "_By the end of this demo, you will be able to:_\n",
    "\n",
    "\n",
    "1. Understand the basics of how to track and manage models with MLflow:\n",
    "    - Train, track, and register a regression model. \n",
    "    - Manage model staging programmatically and using the UI.\n",
    "\n",
    "\n",
    "2. Understand how to serve your model with Mosaic AI Model Serving\n",
    "    - Use the UI to serve your model.\n",
    "    - Query the endpoint using the UI.\n",
    "    - Explore the metrics and logs using the built-in dashboard. \n",
    "\n",
    "3. Introduction to ML With Workflows:\n",
    "    - Use the UI to demonstrate notebook automation for ML tasks via Databricks Workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0428b92-2d83-4ded-b7bc-e86487ac8789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **15.4.x-cpu-ml-scala2.12 15.4.x-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6056b86-6ff4-48b1-b01a-df0123c0cb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "To get into the lesson, we first need to build some data assets and define some configuration variables required for this demonstration. When running the following cell, the output is hidden so our space isn't cluttered. To view the details of the output, you can hover over the next cell and click the eye icon. \n",
    "\n",
    "The cell after the setup, titled `View Setup Variables`, displays the various variables that were created. \n",
    "\n",
    "The classroom setup will create the following assets:  \n",
    "- Catalog (see below for naming)\n",
    "- Database  (see below for naming)\n",
    "- Table `customer_details`\n",
    "- Table `customer_features`\n",
    "- Model (see below for naming)\n",
    "- Model Serving Endpoint **get-started-model-serving-endpoint**\n",
    "\n",
    "This Model Serving Endpoint will be available to everyone and deployed once. Once it is created, you will see the message \"Endpoint 'get-started-model-serving-endpoint' already exists.\" when trying to create it again. This is to limit resource deployment within this workspace. The first creation will take between 5 and 10 minutes. **Do not delete or modify this endpoint in any way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2617cd-9ae5-469f-89ba-e542c6f3e93f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Setup Script"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the catalog \"labuser8027617_1732916382_2w2l_da\"...(1 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/get-started-with-databricks-for-machine-learning/v01\"\n\nValidating the locally installed datasets:\n| listing local files...(0 seconds)\n| validation completed...(0 seconds total)\nCreating & using the catalog \"labuser8027617_1732916382_2w2l_da\"...(2 seconds)\n\nPredefined tables in \"labuser8027617_1732916382_2w2l_da.default\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/labuser8027617_1732916382@vocareum.com/get-started-with-databricks-for-machine-learning\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/get-started-with-databricks-for-machine-learning/v01\n\nSetup completed (8 seconds)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/29 21:53:19 INFO databricks.ml_features._compute_client._compute_client: Setting columns ['customerid'] of table 'labuser8027617_1732916382_2w2l_da.default.customer_features' to NOT NULL.\n2024/11/29 21:53:24 INFO databricks.ml_features._compute_client._compute_client: Setting Primary Keys constraint ['customerid'] on table 'labuser8027617_1732916382_2w2l_da.default.customer_features'.\n2024/11/29 21:53:25 INFO databricks.ml_features._compute_client._compute_client: Created feature table 'labuser8027617_1732916382_2w2l_da.default.customer_features'.\n2024/11/29 21:53:48 INFO mlflow.tracking.fluent: Experiment with name '/Users/labuser8027617_1732916382@vocareum.com/get-started-ml-experiment' does not exist. Creating a new experiment.\n/databricks/python/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312319f9e60646aa962716bfed78ca4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d93718ae0f5483194ae879c5fab7a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/29 21:54:00 INFO mlflow.tracking._tracking_service.client: 🏃 View run end_to_end_ml_on_databricks_run at: dbc-40a12f74-fe09.cloud.databricks.com/ml/experiments/107295324166451/runs/f7de8f6aba3648a49f96c5de3a19b742.\n2024/11/29 21:54:00 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: dbc-40a12f74-fe09.cloud.databricks.com/ml/experiments/107295324166451.\nSuccessfully registered model 'labuser8027617_1732916382_2w2l_da.default.my_model_labuser8027617-1732916382-2w2l-da-gsml'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c892e72123f448fbacc5d261e3d67bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea03ab1df3f948c2ba0728afe16b4491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'labuser8027617_1732916382_2w2l_da.default.my_model_labuser8027617-1732916382-2w2l-da-gsml'.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint 'get-started-model-serving-endpoint' already exists.\nFailed to update permissions: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"labuser8027617_1732916382@vocareum.com does not have Manage permissions on Inference Endpoint with ID: 90c7e88605714e35b03daa6176d2d3c4. Please contact the owner or an administrator for access.\"}\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e707c102-0605-47dd-9c1a-7c4a6921d5e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Setup Variables"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser8027617_1732916382@vocareum.com\nCatalog Name:      labuser8027617_1732916382_2w2l_da\nSchema Name:       default\nWorking Directory: dbfs:/mnt/dbacademy-users/labuser8027617_1732916382@vocareum.com/get-started-with-databricks-for-machine-learning\nUser DB Location:  None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.user_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bee0bc-b307-405d-a7b7-b098939c7128",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Show Tables"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>default</td><td>customer_details</td><td>false</td></tr><tr><td>default</td><td>customer_features</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "default",
         "customer_details",
         false
        ],
        [
         "default",
         "customer_features",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 11
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd2aa337-c8d7-488f-982d-5c700878e21f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Data Preparation for MLflow\n",
    "\n",
    "For this first part, we will read in a customer table, create a feature table, and register that feature table to Databricks Feature Store. This is all performed in the background and was covered in `01 - EDA and Feature Engineering`. Next, we will separate out our features from the target variable and perform a train-test split. \n",
    "\n",
    "Note: We are working with a Pandas DataFrame called `training_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24490f26-689a-43e6-867b-9ec9e4bf66cc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create feature table"
    }
   },
   "outputs": [],
   "source": [
    "# Register feature table to Feature Store and load it\n",
    "training_df = DA.create_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d59d658-40df-414b-8bd4-b35590d98f7c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train-test Split"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use the training dataset to store variables X, the features, and y, the target variable. \n",
    "X = training_df.drop(\"Churn\", axis=1)\n",
    "y = training_df[\"Churn\"]\n",
    "\n",
    "# Convert categorical labels to numerical labels\n",
    "y = y.map({'Yes': 1.0, 'No': 0.0})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbeea794-4b0d-4860-b97d-25a6e30a1a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Part 2: Training And Model Tracking/Management With MLflow\n",
    "\n",
    "Now that we have our training set ready to go, the next step is to train a model using the `sklearn` library. We will build a random forest classification model, tracking the F1-score for a single run. We will initiate the tracking before creating the model using `mlflow.start_run()` as the context manager. Within this manager we will:\n",
    "\n",
    "1. Initialize the random forest classifier\n",
    "2. Fit the model\n",
    "Make a prediction using our test set\n",
    "3. Log the F1-score metric as `test_f1`\n",
    "4. Capture the artifacts for model tracking and management using the flavor `mlflow.sklearn`. *Flavor* in this context simply means that MLflow will package our scikit-learn model in a consistent and standardized way. If we wished to use a different ML library, we would use a different *flavor*.\n",
    "\n",
    "Finally, we will register the model to Unity Catalog. Note, Databricks does not recommend registering your model at the Workspace level. Recall that we did this using the UI in the previous lab `02 - Experimentation with Mosaic AI AutoML`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc76668f-b13a-420b-b0a5-96ee979a195e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Start the MLflow Run"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/29 21:58:30 INFO mlflow.tracking.fluent: Experiment with name '/Users/labuser8027617_1732916382@vocareum.com/M05-End-to-end-ML-Lakehouse' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b4e35e2d0546ebb0a22fef81b714b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f0c73d05b64000bd361c82d262416d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9dd0248e64944dfae9f5233af14feda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae8f12a6b814e50a710d70ea9d83fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de0dc95af72444bb64a5660aa41f0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/29 21:58:52 INFO mlflow.tracking._tracking_service.client: 🏃 View run end_to_end_ml_on_databricks_run at: dbc-40a12f74-fe09.cloud.databricks.com/ml/experiments/107295324166452/runs/45d3d901815848fa99c3dc519105be65.\n2024/11/29 21:58:52 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: dbc-40a12f74-fe09.cloud.databricks.com/ml/experiments/107295324166452.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# set the path for mlflow experiment\n",
    "mlflow.set_experiment(f\"/Users/{DA.username}/M05-End-to-end-ML-Lakehouse\")\n",
    "\n",
    "with mlflow.start_run(run_name = 'end_to_end_ml_on_databricks_run') as run:  \n",
    "    # Initialize the Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples = True,\n",
    "        silent = True\n",
    "    )\n",
    "\n",
    "    mlflow.log_metric(\"test_f1\", f1_score(y_test, y_pred))\n",
    "        \n",
    "    mlflow.sklearn.log_model(\n",
    "        rf_classifier,\n",
    "        artifact_path = \"model-artifacts\", \n",
    "        input_example=X_train[:3],\n",
    "        signature=infer_signature(X_train, y_train)\n",
    "    )\n",
    "\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model-artifacts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a056f0d-1375-463c-bb50-b2e09fcdbedd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Unity Catalog Model Registration"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'labuser8027617_1732916382_2w2l_da.default.my_model_labuser8027617-1732916382-2w2l-da-gsml' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0638ff7e8d914846846e002573a605f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636ceb82a0dd4a4190ba27cbcaeed812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'labuser8027617_1732916382_2w2l_da.default.my_model_labuser8027617-1732916382-2w2l-da-gsml'.\n"
     ]
    }
   ],
   "source": [
    "# Modify the registry uri to point to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Define the model name \n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.my_model_{DA.unique_name('-')}\"\n",
    "\n",
    "# Register the model in the model registry\n",
    "registered_model = mlflow.register_model(model_uri=model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35af0882-2f6f-4893-a1a9-8b20dbcc4369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notice that you will now have an additional version under your model. Navigate to your model in Catalog explorer. You will find version 1 (created during the classroom setup with alias **staging**) and version 2, which you must created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c808b81-c4e0-43a8-97da-67de47fb1fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "# Initialize an MLflow Client\n",
    "client = MlflowClient()\n",
    "\n",
    "# Assign a \"staging\" alias to model version 1\n",
    "client.set_registered_model_alias(\n",
    "    name= registered_model.name,  # The registered model name\n",
    "    alias=\"dev\",  # The alias representing the dev environment\n",
    "    version=registered_model.version  # The version of the model you want to move to \"dev\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15f52ba-bda5-485c-9d81-bd893b1be59a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Note🚨 : Below instructions are only for demonstration purposes. Please do not provision an endpoint. It is already created during the workspace setup.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d62051d-fc3c-44d6-9fe8-269348786385",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Part 3: Mosaic AI Model Serving\n",
    "\n",
    "**Setting Up Model Serving**\n",
    "\n",
    "We can create Model Serving endpoints with the Databricks Machine Learning API or the Databricks Machine Learning UI. An endpoint can serve any registered Python MLflow model in the **Model Registry**.\n",
    "\n",
    "In order to keep it simple, in this demo, we are going to use the Model Serving UI for creating, managing and using the Model Serving endpoints. We can create model serving endpoints with the **\"Serving\"** page UI or directly from registered **\"Models\"** page.  \n",
    "\n",
    "Let's go through the steps of creating a model serving endpoint in Models page. **You will not actually create the endpoint.**\n",
    "\n",
    "- Go to **Models**. \n",
    "\n",
    "- Select **Unity Catalog** at the top and select **Owned by me** as well.\n",
    "\n",
    "- Select the model you want to serve under the **Name** column. Notice this will take you to the Catalog menu. \n",
    "\n",
    "- Click the **Serve this model** button on the top right. This will take you to the **Serving endpoints** screen.\n",
    "\n",
    "- Next in **General**, enter in a name. This name should be unique like your first and last name. For example, **get-started-model-serving-endpoint**.\n",
    "\n",
    "- There are several configurations under **Served entities** that we will not discuss here. Leave **Entity** and **Compute type** to default values. For **Compute scale-out**, select **small**. You can select **Scale to zero** for this lesson as well. We will be deleting the endpoint at the end of this lesson, so this doesn't matter too much for our purposes. \n",
    "\n",
    "- **Do not click Create** at the bottom right. The above instructions are only for demonstration purposes. **Do not provision an endpoint.**\n",
    "\n",
    "    - If you happen to accidentally create an endpoint, you can navigate to the left side bar and click on **Serving**. Then click on the model you began provisioning and click on the 3 vertical dots at the top right. Select **Delete**. Again, **Do not provision an endpoint.**\n",
    "\n",
    "- If you do click create, you will be met with an error saying \"Endpoint with name 'get-started-model-serving-endpoint' already exists.\" This is because we already setup this endpoint during the setup to this course. **Do not provision an endpoint by changing the name.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41ba6dd1-7a29-4bf6-8936-3e28aedd2020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Model Serving Endpoint with Python\n",
    "\n",
    "**Note**:  We will not be creating a new endpoint in this **demonstration**. However, the code implementation is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d512b31-348c-49cb-8ccc-6d79c7f8edc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "```\n",
    "\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "endpoint_name = f\"{DA.username}-model\"\n",
    "endpoint_name = endpoint_name.replace(\"@databricks.com\", \"\").replace('.', '-')\n",
    "\n",
    "# Check if the endpoint already exists\n",
    "try:\n",
    "    # Attempt to get the endpoint\n",
    "    existing_endpoint = client.get_endpoint(endpoint_name)\n",
    "    print(f\"Endpoint '{endpoint_name}' already exists.\")\n",
    "except Exception as e:\n",
    "    # If not found, create the endpoint\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        print(f\"Creating a new endpoint: {endpoint_name}\")\n",
    "        endpoint = client.create_endpoint(\n",
    "            name=endpoint_name,\n",
    "            config={\n",
    "                \"served_entities\": [\n",
    "                    {\n",
    "                        \"name\": \"my-model\",\n",
    "                        \"entity_name\": model_name,\n",
    "                        \"entity_version\": \"1\",\n",
    "                        \"workload_size\": \"Small\",\n",
    "                        \"scale_to_zero_enabled\": True\n",
    "                    }\n",
    "                ],\n",
    "                \"traffic_config\": {\n",
    "                    \"routes\": [\n",
    "                        {\n",
    "                            \"served_model_name\": \"my-model\",\n",
    "                            \"traffic_percentage\": 100\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7ed404c-3286-493b-b558-86a0ba05d319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Query Serving Endpoint\n",
    "\n",
    "Let's use the deployed model for real-time inference. Here’s a step-by-step guide for querying an endpoint in Databricks Model Serving:\n",
    "\n",
    "- Go to the **Serving** endpoints page and select the endpoint you want to query.\n",
    "\n",
    "- Click **Use** button the top right corner.\n",
    "\n",
    "- There are 4 methods for querying an endpoint; **browser**, **CURL**, **Python**, and **SQL**. For now, let's use the easiest method; querying right in the **browser** window. In this method, we need to provide the input parameters in JSON format. Since we used `mlflow.sklearn.autolog()` with `log_input_examples = True`, we registered an example with MLflow, which appear automatically when selecting **browser**.\n",
    "\n",
    "- Click **Send request**.\n",
    "\n",
    "- **Response** field on the right panel will show the result of the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b72102d-cde2-4565-9f20-148fc6ea7d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 4: ML With Workflows\n",
    "\n",
    "### Creating a Job\n",
    "\n",
    "Now, let's create a workflow job to run this notebook:\n",
    "\n",
    "1. Start by navigating to **Workflows** on the left sidebar.\n",
    "   \n",
    "2. Click **Create job** at the top right.\n",
    "\n",
    "3. In the presented menu, enter a name under **Task Name**. For example, you can use **test_task_name**.\n",
    "\n",
    "4. Under **Type**, ensure that **Notebook** is selected. Also, make sure **Workspace** is selected under **Source**. Both should be selected by default.\n",
    "\n",
    "5. Next, we will attach this notebook to the job. Go to **Path**. Click the drop-down menu. You will be presented with a menu that will allow you to navigate to this notebook. By default, you will already be in your home workspace. It will look like `labuserXXXXXXX@vocareum.com`. Click the source folder (there will only be a single option listed other than **Trash**). Select `M02 - Databricks for Machine Learning` and then select `03 - Getting Started with MLflow and Mosaic AI Model Serving`. Click **Confirm** at the bottom right of the **Select Notebook** menu. \n",
    "\n",
    "6. Under **Compute**, you'll see **Serverless** is automatically selected.\n",
    "Use the drop-down menu to select your existing All-Purpose **cluster** (DBR 15.4 LTS ML).\n",
    "\n",
    "7. You’ll see additional options, such as **dependent libraries**, **parameters**, **notifications**, **retries**, and **duration threshold**. You can leave these options unpopulated for this lesson. \n",
    "\n",
    "8. Click **Create Task**. A message will appear at the top right indicating your task was successfully created.\n",
    "\n",
    "9. In the right menu, there are various options. For example, you can add tags, set job parameters, and configure schedules for runs. We'll leave these alone for this lesson.\n",
    "\n",
    "10. Finally, to give the job a name, double-click the title at the top left and enter an appropriate name. For example, `Workflow1`.\n",
    "\n",
    "11. When you are ready to run the job, click **Run Now** at the top right. A note will appear at the top right indicating a run has been initiated. Note that due to the clean-up in the next cell, your data assets will be missing when inspecting your catalog after the job finishes. \n",
    "    - You can also run this workflow from the parent menu (navigate to **Workflows** on the sidebar within Databricks) by clicking the **Run now** button ▶. \n",
    "\n",
    "### Inspect Your Run\n",
    "\n",
    "After your job completes its run, you can inspect it:\n",
    "\n",
    "1. Select **Workflows** again on the sidebar menu. \n",
    "\n",
    "2. Click on the name of the job you just created. \n",
    "\n",
    "3. There are a few ways to view the notebook you just ran. You can do any of the following and it will take you to the same location:\n",
    "    - Click on **go to the latest successful run**.\n",
    "    - Click on the date under **Start time**.\n",
    "    - Click on the green bar (meaning a successful run) displayed within the diagram. If the bar is red, that means your job failed, but you can still inspect the notebook that ran. This option will also display the workflow configuration you setup previously. \n",
    "\n",
    "4. This will take you to a static copy of the notebook. You cannot edit it, but you can view the outputs of each cell. Confirm that all cells have run successfully. \n",
    "\n",
    "Of course, this is a simple example of creating a job with a single notebook. In practice, we use the various options mentioned earlier to tie together complex pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77345ca5-1af6-4193-8cb6-98721c3382d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Classroom Clean-up\n",
    "\n",
    "After completing the demo, it's important to clean up any resources that were created.\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05091a21-197b-411a-ae70-fbd812dba1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the catalog \"labuser8027617_1732916382_2w2l_da\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(0 seconds)\n| validation completed...(0 seconds total)\n"
     ]
    }
   ],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "144ad867-74c5-4ba8-81e8-234e3f53ed07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusions And Next Steps\n",
    "\n",
    "In this demo, we covered how to track, manage, and stage models with MLflow, as well as serve models using Mosaic AI Model Serving. You learned to automate model deployment workflows by parameterizing variables in the UI, streamlining the pipeline from model training to inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ea751d-1b61-461c-853d-0230e136ddad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 107295324166104,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03 - Getting Started with MLflow and Mosaic AI Model Serving",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}