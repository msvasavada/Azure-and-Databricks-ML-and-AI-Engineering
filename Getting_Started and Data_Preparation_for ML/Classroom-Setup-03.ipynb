{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6e9fe1d-8867-4351-934c-d2db05db7377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58bd9484-cc89-4631-9474-efb9ebf3f345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a sample table to be used for the lesson\n",
    "def create_details_table(self):\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    df_customer_details = spark.read.csv(\n",
    "        f\"{DA.paths.datasets}/telco/customer-details.csv\", header=True, inferSchema=True\n",
    "    ).withColumnRenamed(\"CustomerID\", \"id\")\n",
    "\n",
    "    # save to catalog\n",
    "    spark.sql(f\"USE CATALOG {DA.catalog_name}\")\n",
    "    df_customer_details.write.mode(\"overwrite\").saveAsTable(\"customer_details\")\n",
    "\n",
    "\n",
    "DBAcademyHelper.monkey_patch(create_details_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef54908-998c-4391-97dd-419da89d1580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_customer_data(self):\n",
    "    import pyspark.pandas as ps\n",
    "    import re\n",
    "    from pyspark.sql.functions import col\n",
    "    from databricks.feature_store import FeatureStoreClient\n",
    "    fs = FeatureStoreClient()\n",
    "\n",
    "    # Read data sets and join them by customer id\n",
    "    sdf_customer_demographics = spark.read.csv(\n",
    "        f\"{DA.paths.datasets}/telco/customer-demographics.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    sdf_customer_details = spark.read.csv(\n",
    "        f\"{DA.paths.datasets}/telco/customer-details.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    ).withColumnRenamed(\"customerID\", \"id\")\n",
    "\n",
    "    # Join the two datasets on customerID\n",
    "    sdf_customers = sdf_customer_demographics.join(\n",
    "        sdf_customer_details, col(\"customerID\") == col(\"id\")\n",
    "    )\n",
    "\n",
    "    # Convert Spark DataFrame to pandas-on-Spark DataFrame\n",
    "    df_customers = ps.DataFrame(sdf_customers)\n",
    "\n",
    "    # Exclude columns that are not needed\n",
    "    df_customers = df_customers.drop(\n",
    "        columns=[\n",
    "            \"Dependents\",\n",
    "            \"id\",\n",
    "            \"MultipleLines\",\n",
    "            \"OnlineSecurity\",\n",
    "            \"OnlineBackup\",\n",
    "            \"DeviceProtection\",\n",
    "            \"TechSupport\",\n",
    "            \"PaperlessBilling\",\n",
    "            \"TotalCharges\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    df_customers_ohe = ps.get_dummies(\n",
    "        df_customers,\n",
    "        columns=[\n",
    "            \"gender\",\n",
    "            \"SeniorCitizen\",\n",
    "            \"Partner\",\n",
    "            \"PhoneService\",\n",
    "            \"InternetService\",\n",
    "            \"StreamingTV\",\n",
    "            \"StreamingMovies\",\n",
    "            \"Contract\",\n",
    "            \"PaymentMethod\",\n",
    "        ],\n",
    "        dtype=\"float64\",\n",
    "    )\n",
    "\n",
    "    # Convert integer columns to float64 (best practice for MLflow)\n",
    "    df_customers_ohe['tenure'] = df_customers_ohe['tenure'].astype('float64')\n",
    "\n",
    "    # Clean up column names by replacing non-alphanumeric characters with underscores and lowercasing them\n",
    "    df_customers_ohe.columns = [\n",
    "        re.sub(r\"[^a-zA-Z0-9_]\", \"_\", col).lower() for col in df_customers_ohe.columns\n",
    "    ]\n",
    "    \n",
    "    #drop table if exists\n",
    "    try:\n",
    "        fs.drop_table('customer_features')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # exclude prediction column and save the features to the feature table\n",
    "    df_customers_ohe = df_customers_ohe.drop(columns=[\"churn\"])\n",
    "\n",
    "    fs.create_table(\n",
    "    name = \"customer_features\",\n",
    "    primary_keys = [\"customerid\"],\n",
    "    schema = df_customers_ohe.spark.schema(),\n",
    "    description=\"This customer-level table contains one-hot encoded and numeric features.\"\n",
    "    )\n",
    "\n",
    "    # Set catalog\n",
    "    spark.sql(f\"USE CATALOG {DA.catalog_name}\")\n",
    "\n",
    "    fs.write_table(df=df_customers_ohe.to_spark(), name=\"customer_features\", mode=\"overwrite\")\n",
    "\n",
    "# Register the function to the DBAcademyHelper class\n",
    "DBAcademyHelper.monkey_patch(preprocess_customer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b8a42d-a3f3-469c-a262-c1c84a6d000c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureLookup, FeatureStoreClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "def create_feature_store(self):\n",
    "    fs = FeatureStoreClient()\n",
    "\n",
    "    # Read in customer_details table as a Spark DataFrame\n",
    "    sdf_lookup = spark.read.table('customer_details').select(['id','Churn'])\n",
    "\n",
    "    feature_lookups = [\n",
    "        FeatureLookup(\n",
    "        table_name = 'customer_features',\n",
    "        lookup_key = 'id'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    training_set = fs.create_training_set(\n",
    "        df = sdf_lookup,\n",
    "        feature_lookups = feature_lookups,\n",
    "        label = 'Churn',\n",
    "        exclude_columns = ['id']\n",
    "    )\n",
    "\n",
    "    training_df = training_set.load_df().toPandas()\n",
    "\n",
    "    return training_df \n",
    "    \n",
    "DBAcademyHelper.monkey_patch(create_feature_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd75adfd-fd39-4b1f-ad18-17ee012fc408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.deployments import get_deploy_client\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def create_model(self):\n",
    "    fs = FeatureStoreClient()\n",
    "\n",
    "    # Read in customer_details table as a Spark DataFrame\n",
    "    sdf_lookup = spark.read.table('customer_details').select(['id','Churn'])\n",
    "\n",
    "    feature_lookups = [\n",
    "        FeatureLookup(\n",
    "        table_name = 'customer_features',\n",
    "        lookup_key = 'id'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    training_set = fs.create_training_set(\n",
    "        df = sdf_lookup,\n",
    "        feature_lookups = feature_lookups,\n",
    "        label = 'Churn',\n",
    "        exclude_columns = ['id']\n",
    "    )\n",
    "\n",
    "    training_df = training_set.load_df().toPandas()\n",
    "\n",
    "\n",
    "\n",
    "    # Use the training dataset to store variables X, the features, and y, the target variable. \n",
    "    X = training_df.drop(\"Churn\", axis=1)\n",
    "    y = training_df[\"Churn\"]\n",
    "\n",
    "    # Convert categorical labels to numerical labels\n",
    "    y = y.map({'Yes': 1.0, 'No': 0.0})\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # set the path for mlflow experiment\n",
    "    mlflow.set_experiment(f\"/Users/{DA.username}/get-started-ml-experiment\")\n",
    "\n",
    "    with mlflow.start_run(run_name = 'end_to_end_ml_on_databricks_run') as run:  \n",
    "        # Initialize the Random Forest classifier\n",
    "        rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "        # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "        mlflow.sklearn.autolog(\n",
    "            log_input_examples = True,\n",
    "            silent = True\n",
    "        )\n",
    "\n",
    "        mlflow.log_metric(\"test_f1\", f1_score(y_test, y_pred))\n",
    "            \n",
    "        mlflow.sklearn.log_model(\n",
    "            rf_classifier,\n",
    "            artifact_path = \"model-artifacts\", \n",
    "            input_example=X_train[:3],\n",
    "            signature=infer_signature(X_train, y_train)\n",
    "        )\n",
    "\n",
    "        model_uri = f\"runs:/{run.info.run_id}/model-artifacts\"\n",
    "\n",
    "    # Modify the registry uri to point to Unity Catalog\n",
    "    mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "    # Define the model name \n",
    "    model_name = f\"{DA.catalog_name}.{DA.schema_name}.my_model_{DA.unique_name('-')}\"\n",
    "\n",
    "    # Register the model in the model registry\n",
    "    registered_model = mlflow.register_model(model_uri=model_uri, name=model_name)\n",
    "\n",
    "    # Initialize an MLflow Client\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # Assign a \"staging\" alias to model version 1\n",
    "    client.set_registered_model_alias(\n",
    "        name= registered_model.name,  # The registered model name\n",
    "        alias=\"staging\",  # The alias representing the staging environment\n",
    "        version= 1 # The version of the model you want to move to \"staging\"\n",
    "    )\n",
    "\n",
    "    client = get_deploy_client(\"databricks\")\n",
    "    endpoint_name = f\"get-started-model-serving-endpoint\"\n",
    "    endpoint_name = endpoint_name.replace(\"@databricks.com\", \"\").replace('.', '-')\n",
    "\n",
    "    try:\n",
    "        # Attempt to get the endpoint\n",
    "        existing_endpoint = client.get_endpoint(endpoint_name)\n",
    "        print(f\"Endpoint '{endpoint_name}' already exists.\")\n",
    "    except Exception as e:\n",
    "        # If not found, create the endpoint\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "            print(f\"Creating a new endpoint: {endpoint_name}\")\n",
    "            endpoint = client.create_endpoint(\n",
    "                name=endpoint_name,\n",
    "                config={\n",
    "                    \"served_entities\": [\n",
    "                        {\n",
    "                            \"name\": \"my-model\",\n",
    "                            \"entity_name\": model_name,\n",
    "                            \"entity_version\": \"1\",\n",
    "                            \"workload_size\": \"Small\",\n",
    "                            \"scale_to_zero_enabled\": True\n",
    "                        }\n",
    "                    ],\n",
    "                    \"traffic_config\": {\n",
    "                        \"routes\": [\n",
    "                            {\n",
    "                                \"served_model_name\": \"my-model\",\n",
    "                                \"traffic_percentage\": 100\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    #update permissions so all users can query the endpoint\n",
    "\n",
    "    client = get_deploy_client(\"databricks\")\n",
    "    endpoint_id = client.get_endpoint(endpoint=endpoint_name).id\n",
    "    DATABRICKS_INSTANCE = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    # API endpoint for retrieving permissions\n",
    "    url = f\"{DATABRICKS_INSTANCE}/api/2.0/permissions/serving-endpoints/{endpoint_id}\"\n",
    "\n",
    "    # Headers for the request\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Payload to set permissions for the 'users' group\n",
    "    payload = {\n",
    "        \"access_control_list\": [\n",
    "            {\n",
    "                \"group_name\": \"users\",\n",
    "                \"permission_level\": \"CAN_QUERY\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send the PATCH request to update permissions\n",
    "    response = requests.patch(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Permissions updated successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to update permissions: {response.text}\")\n",
    "\n",
    "DBAcademyHelper.monkey_patch(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d0f8883-a596-4d91-a10a-498f38b57e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA = DBAcademyHelper(course_config, lesson_config)  # Create the DA object\n",
    "DA.reset_lesson()                                   # Reset the lesson to a clean state\n",
    "DA.init()                                           # Performs basic intialization including creating schemas and catalogs\n",
    "DA.conclude_setup()                                 # Finalizes the state and prints the config for the student\n",
    "\n",
    "# Create customer_details table\n",
    "DA.create_details_table()\n",
    "\n",
    "# Create feature table\n",
    "DA.preprocess_customer_data()\n",
    "\n",
    "# Create model and serving endpoint for instruction\n",
    "DA.create_model()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Classroom-Setup-03",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}