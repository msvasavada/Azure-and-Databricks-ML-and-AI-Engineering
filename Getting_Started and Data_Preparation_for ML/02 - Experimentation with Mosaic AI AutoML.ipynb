{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee7a5ce2-2bcb-4701-9217-7e5cd91edba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "661da46d-f344-4f1c-8908-4dceb329d6b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Experimentation with Mosaic AI AutoML\n",
    "\n",
    "In this demo, we will explore the powerful capabilities of AutoML (Automated Machine Learning) within Databricks. AutoML allows you to automatically build, train, and select the best machine learning models for your data without manual intervention. We will create an AutoML experiment, evaluate the results, register the best model, and transition it to the **Staging** stage.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "_By the end of this lesson, you will be able to:_\n",
    "\n",
    "1. **Establish a baseline champion model with AutoML:**\n",
    "    - Create an AutoML experiment using the Experiments UI.\n",
    "    - (Optional) Create an AutoML experiment using the `databricks` Python library. \n",
    "    - Understand the various configuration options available in AutoML experiments.\n",
    "2. **Model and notebook inspection:**\n",
    "    - Evaluate the results of an AutoML experiment using the UI and identify the best model, called the champion model.\n",
    "    - (Optional) Evaluate the results of an AutoML experiment using a notebook and identify the champion model.  \n",
    "    - Open and explore the automatically generated notebook for the champion model. \n",
    "3. **Register the champion model:**\n",
    "    - Register the model at the Workspace level. \n",
    "    - Register the model within Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7b6262f-b719-47ff-adf4-0e63e3eaf128",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **15.4.x-cpu-ml-scala2.12 15.4.x-scala2.12**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aad61dc-7666-4721-a460-fa201f97b51d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "To get into the lesson, we first need to build some data assets and define some configuration variables required for this demonstration. When running the following cell, the output is hidden so our space isn't cluttered. To view the details of the output, you can hover over the next cell and click the eye icon. \n",
    "\n",
    "The cell after the setup, titled `View Setup Variables`, displays the various variables that were created. You can click the Catalog icon in the notebook space to the right to see that your catalog was created with no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e998ba1-6059-4fa2-884c-cc1e2c4981ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run Setup Script"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the catalog \"labuser8027617_1732830335_zsw6_da\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/get-started-with-databricks-for-machine-learning/v01\"\n\nValidating the locally installed datasets:\n| listing local files...(0 seconds)\n| validation completed...(0 seconds total)\nCreating & using the catalog \"labuser8027617_1732830335_zsw6_da\"...(1 seconds)\n\nPredefined tables in \"labuser8027617_1732830335_zsw6_da.default\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/labuser8027617_1732830335@vocareum.com/get-started-with-databricks-for-machine-learning\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/get-started-with-databricks-for-machine-learning/v01\n\nSetup completed (3 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf22af9-e08e-4c0f-80f0-2c662afede14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Setup Variables"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser8027617_1732830335@vocareum.com\nCatalog Name:      labuser8027617_1732830335_zsw6_da\nSchema Name:       default\nWorking Directory: dbfs:/mnt/dbacademy-users/labuser8027617_1732830335@vocareum.com/get-started-with-databricks-for-machine-learning\nUser DB Location:  None\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.user_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db0ffc68-faad-43b3-8cea-afbd06782015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Prepare Data\n",
    "\n",
    "For this demonstration, we will utilize a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses customer demographics, including gender, as well as internet subscription details such as subscription plans and payment methods.\n",
    "\n",
    "To get started, execute the code block below. \n",
    "\n",
    "This will create the `customers` table and allow us to explore its features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d1db1c-a64b-474e-acd9-7a401610d58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- CustomerID: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- SeniorCitizen: integer (nullable = true)\n |-- PhoneService: string (nullable = true)\n |-- MultipleLines: string (nullable = true)\n |-- PaymentMethod: string (nullable = true)\n |-- MonthlyCharges: double (nullable = true)\n |-- Churn: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "DA.create_customers_table()\n",
    "spark.table('customers').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d310ff1-e1d2-4b6a-a2f6-3d8bdeef58b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Create An AutoML Experiment\n",
    "\n",
    "Here we will discuss two methods for creating an AutomL experiment:\n",
    "1. Using the Databricks UI. \n",
    "2. (Optional) Programmatically using the `databricks` Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bee5537-3cb4-4924-a111-fa49401436fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create An AutoML Experiment With The UI\n",
    "\n",
    "Let's initiate an AutoML experiment to construct a baseline model for predicting customer churn. The target field for this prediction will be the `Churn` field.\n",
    "\n",
    "Follow these step-by-step instructions to create an AutoML experiment:\n",
    "\n",
    "1. Navigate to **Experiments** under **Machine Learning** in the left sidebar menu.\n",
    "\n",
    "2. Click on **Create AutoML experiment** located at the top.\n",
    "\n",
    "3. Choose a cluster to execute the experiment.\n",
    "\n",
    "4. For the ML problem type, opt for **Classification**.\n",
    "\n",
    "5. To select the `customers` table as the input training data, select `Browse` under `Input training dataset` and navigate to the catalog and `default` database that was created previously. Alternatively, you type in your name in Catalogs within the **Select training data** menu to find your table. \n",
    "\n",
    "6. Specify **`Churn`** as the **Prediction target**.\n",
    "\n",
    "7. Deselect the **CustomerID** field as it's not needed as a feature.\n",
    "\n",
    "8. In **Experiment name** you will see an automatically generated string like `Churn_customers-2024_10_16-10_20`. Append this string with your first and last name or some other unique identifier like  `Churn_customers-2024_10_16-10_20_firstName_lastName`. If a message appears that indicates the name is taken, try a different set of string values to append to the experiment name. \n",
    "\n",
    "9. In the **Advanced Configuration** section, set the **Timeout** to **5 minutes**.\n",
    "\n",
    "10. Click on **Start AutoML**. \n",
    "\n",
    "**Optional Advanced Configuration:**\n",
    "\n",
    "- You have the flexibility to choose the **evaluation metric** and your preferred **training framework**.\n",
    "\n",
    "- If your dataset includes a timeseries field, you can define it when splitting the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "843dd103-7085-4710-8f09-432b53e0a984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (Optional) Create An AutoML Experiment Within A Notebook\n",
    "\n",
    "We can Programmatically kickoff an AutoML experiment using the `databricks` Python library as well. After clicking **Run** on the following cell, you can go over to **Experiments** on the left menu bar and navigate to the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3caec241-0781-4686-8a40-e284865e68d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/28 22:25:24 INFO databricks.automl.client.manager: AutoML will optimize for F1 score metric, which is tracked as val_f1_score in the MLflow experiment.\n2024/11/28 22:25:26 INFO databricks.automl.client.manager: MLflow Experiment ID: 4412028206669628\n2024/11/28 22:25:26 INFO databricks.automl.client.manager: MLflow Experiment: https://dbc-40a12f74-fe09.cloud.databricks.com/?o=2972073332228503#mlflow/experiments/4412028206669628\n2024/11/28 22:27:02 INFO databricks.automl.client.manager: Data exploration notebook: https://dbc-40a12f74-fe09.cloud.databricks.com/?o=2972073332228503#notebook/4412028206669646\n2024/11/28 22:31:22 INFO databricks.automl.client.manager: AutoML experiment completed successfully.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <style>\n",
       "            .grid-container {\n",
       "              display: grid\n",
       "              grid-template-columns: auto;\n",
       "              padding: 10px;\n",
       "            }\n",
       "            <!-- Picked to be same as in webapp/web/js/templates/iframeSandbox.css -->\n",
       "            .grid-container div {\n",
       "              font-family: Helvetica, Arial, sans-serif;\n",
       "              font-size: 14px;\n",
       "            }\n",
       "        </style>\n",
       "        <div class=\"grid-container\">\n",
       "            \n",
       "            <div><p>For exploratory data analysis, open the <a href=https://dbc-40a12f74-fe09.cloud.databricks.com/?o=2972073332228503#notebook/4412028206669646>data exploration notebook</a></p></div>\n",
       "            <div><p>To view the best performing model, open the <a href=#notebook/4412028206669654>best trial notebook</a></p></div>\n",
       "            <div><p>To view details about all trials, navigate to the <a href=https://dbc-40a12f74-fe09.cloud.databricks.com/?o=2972073332228503#mlflow/experiments/4412028206669628>MLflow experiment</a></p></div>\n",
       "            <div><p><strong>Metrics for the best trial:</strong></p></div>\n",
       "            <div>\n",
       "                <!-- class inlined from webapp/web/js/templates/iframeSandbox.css -->\n",
       "                \n",
       "                    <table class=\"dataframe\">\n",
       "                        <thead>\n",
       "                          <tr>\n",
       "                            <th></th>\n",
       "                            <th>Train</th>\n",
       "                            <th>Validation</th>\n",
       "                            <th>Test</th>\n",
       "                          </tr>\n",
       "                        </thead>\n",
       "                        <tbody>\n",
       "                        \n",
       "                <tr>\n",
       "                    <th> f1_score </th>\n",
       "                    <td> 0.141 </td>\n",
       "                    <td> 0.080 </td>\n",
       "                    <td> 0.174 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> recall_score </th>\n",
       "                    <td> 0.082 </td>\n",
       "                    <td> 0.045 </td>\n",
       "                    <td> 0.105 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> roc_auc </th>\n",
       "                    <td> 0.716 </td>\n",
       "                    <td> 0.718 </td>\n",
       "                    <td> 0.731 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> false_negatives </th>\n",
       "                    <td> 1003.000 </td>\n",
       "                    <td> 378.000 </td>\n",
       "                    <td> 325.000 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> false_positives </th>\n",
       "                    <td> 78.000 </td>\n",
       "                    <td> 35.000 </td>\n",
       "                    <td> 36.000 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> example_count </th>\n",
       "                    <td> 4168.000 </td>\n",
       "                    <td> 1436.000 </td>\n",
       "                    <td> 1372.000 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> precision_score </th>\n",
       "                    <td> 0.533 </td>\n",
       "                    <td> 0.340 </td>\n",
       "                    <td> 0.514 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> true_positives </th>\n",
       "                    <td> 89.000 </td>\n",
       "                    <td> 18.000 </td>\n",
       "                    <td> 38.000 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> precision_recall_auc </th>\n",
       "                    <td> 0.438 </td>\n",
       "                    <td> 0.425 </td>\n",
       "                    <td> 0.449 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> true_negatives </th>\n",
       "                    <td> 2998.000 </td>\n",
       "                    <td> 1005.000 </td>\n",
       "                    <td> 973.000 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> log_loss </th>\n",
       "                    <td> 0.522 </td>\n",
       "                    <td> 0.537 </td>\n",
       "                    <td> 0.519 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> score </th>\n",
       "                    <td> 0.741 </td>\n",
       "                    <td> 0.712 </td>\n",
       "                    <td> 0.737 </td>\n",
       "                </tr>\n",
       "                \n",
       "\n",
       "                <tr>\n",
       "                    <th> accuracy_score </th>\n",
       "                    <td> 0.741 </td>\n",
       "                    <td> 0.712 </td>\n",
       "                    <td> 0.737 </td>\n",
       "                </tr>\n",
       "                \n",
       "                        </tbody>\n",
       "                    </table>\n",
       "            \n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks import automl\n",
    "from datetime import datetime\n",
    "# Define parameters for the classify function\n",
    "table_name = \"customers\" # The table containing our features and labels\n",
    "target_col = \"Churn\" # The variable we are trying to predict\n",
    "exclude_cols = [\"CustomerID\"] # Exclude the CustomerID column\n",
    "timeout_minutes = 5  # The maximum time in minutes to run the experiment\n",
    "\n",
    "\n",
    "# Run the AutoML experiment to generate the best classify model and generate the best run notebook\n",
    "automl_run = automl.classify(\n",
    "    dataset=spark.table(table_name), \n",
    "    target_col=target_col, \n",
    "    exclude_cols=exclude_cols, \n",
    "    timeout_minutes=timeout_minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed969f8-71a7-4177-9bc3-eaa1c3df176f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Best Run Inspection\n",
    "\n",
    "Next, we will inspect the best run produced by AutoML:\n",
    "\n",
    "1. Using the Databricks UI. \n",
    "2. (Optional) Programmatically using the `databricks` Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3e93ab5-d3c3-4e96-b153-33dde7b8e2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Best Run Inspection With The UI\n",
    "\n",
    "Once the experiment is finished, it's time to examine the best run:\n",
    "\n",
    "1. Access the completed experiment in the **Experiments** section.\n",
    "\n",
    "2. Identify the best model run by evaluating the displayed **metrics**. Alternatively, you can click on **View notebook for the best model** to access the automatically generated notebook for the top-performing model.\n",
    "\n",
    "3. Utilize the **Chart** tab to compare and contrast the various models generated during the experiment.\n",
    "\n",
    "You can find all details for the run  on the experiment page. There are different columns such as the framework used (e.g., Scikit-Learn, XGBoost), evaluation metrics (e.g., Accuracy, F1 Score), and links to the corresponding notebooks for each model. This allows you to make informed decisions about selecting the best model for your specific use case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1134ae15-2b3d-441e-b2bc-d6931c9922a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### (Optional) Best Run Inspection Within A Notebook \n",
    "\n",
    "Alternatively, we can approach the inspection programmatically. First, we will display all the experiments only we have created. Second, we will create Pandas DataFrame that contains F1-score information along with a visual to help pick the champion model. \n",
    "\n",
    "**Notebooks for Other Experiment Trials in AutoML**\n",
    "\n",
    "For classification and regression experiments, AutoML generated notebooks for data exploration and the best trial in your experiment are automatically imported to your workspace. For all trials besides the best trial, the notebooks **are NOT created** automatically. If you need to use these notebooks, you can manually import them into your workspace with the **`automl.import_notebook`** Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bd804c-c081-42e9-bf46-e3e8b6734e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment ID: 4412028206669628, \nName: /Users/labuser8027617_1732830335@vocareum.com/databricks_automl/Churn_global_temp.automl_17c9cec7_1366_44dc_880f_400e2dedf56b_2024-25-28_22-25-24, \nArtifact Location: dbfs:/databricks/mlflow-tracking/4412028206669628\n\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Initialize MLflow client\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# List all experiments using search_experiments()\n",
    "experiments = client.search_experiments()\n",
    "\n",
    "# Loop through experiments and check if the username is part of the experiment name or artifact location\n",
    "for experiment in experiments:\n",
    "    if DA.username in experiment.name or DA.username in experiment.artifact_location:\n",
    "        print(f\"Experiment ID: {experiment.experiment_id}, \\nName: {experiment.name}, \\nArtifact Location: {experiment.artifact_location}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "481548ef-6384-4bc4-b826-a403f5f7c0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's grab only the latest experiment, assuming that's the run we want. You can modify this code to grab a specific experiment if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21646321-0619-459d-8019-e99791e1d685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest experiment name: /Users/labuser8027617_1732830335@vocareum.com/databricks_automl/Churn_global_temp.automl_17c9cec7_1366_44dc_880f_400e2dedf56b_2024-25-28_22-25-24\n"
     ]
    }
   ],
   "source": [
    "for experiment in experiments:\n",
    "    if DA.username in experiment.name or DA.username in experiment.artifact_location:\n",
    "        latest_experiment_name = experiment.name\n",
    "        break\n",
    "print(f\"Latest experiment name: {latest_experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9a97231-c3cb-419b-a918-689ad4ee9269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now that we have the name of our latest experiment, we can construct a Pandas DataFrame that grabs the F1-score for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e10e83f-b1e7-4545-a543-42e49228cb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>run_id</th><th>train_f1_score</th><th>validation_f1_score</th><th>test_f1_score</th></tr></thead><tbody><tr><td>f486e965bf2a41c4a3afdd0746d5c199</td><td>0.14138204924543288</td><td>0.0801781737193764</td><td>0.17391304347826084</td></tr><tr><td>ed53366b4b3a4b11b4fbff89225614c1</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>cf25dab8799f4473aa7b4903a381e714</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>cc0f3b9bc8b04c78b83dcc3961c5373d</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>d65f89a34d7f41cabe8c90c9bd14a042</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "f486e965bf2a41c4a3afdd0746d5c199",
         0.14138204924543288,
         0.0801781737193764,
         0.17391304347826084
        ],
        [
         "ed53366b4b3a4b11b4fbff89225614c1",
         0.0,
         0.0,
         0.0
        ],
        [
         "cf25dab8799f4473aa7b4903a381e714",
         0.0,
         0.0,
         0.0
        ],
        [
         "cc0f3b9bc8b04c78b83dcc3961c5373d",
         0.0,
         0.0,
         0.0
        ],
        [
         "d65f89a34d7f41cabe8c90c9bd14a042",
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "train_f1_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "validation_f1_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "test_f1_score",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the experiment by name\n",
    "experiment = mlflow.get_experiment_by_name(latest_experiment_name)\n",
    "\n",
    "# Initialize an empty list to store the data for the DataFrame\n",
    "data = []\n",
    "\n",
    "if experiment:\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "    # Step 2: Retrieve all runs from the experiment\n",
    "    runs = mlflow.search_runs(experiment_ids=experiment_id)\n",
    "    \n",
    "    # Step 3: Access the F1 scores for training, validation, and test data\n",
    "    for _, run in runs.iterrows():\n",
    "        run_id = run['run_id']\n",
    "        \n",
    "        # Fetch the run data\n",
    "        run_data = mlflow.get_run(run_id)\n",
    "        \n",
    "        # Fetch F1 scores\n",
    "        train_f1_score = run_data.data.metrics.get('training_f1_score', None)\n",
    "        val_f1_score = run_data.data.metrics.get('val_f1_score', None)\n",
    "        test_f1_score = run_data.data.metrics.get('test_f1_score', None)\n",
    "        \n",
    "        # Append the data for this run to the list\n",
    "        data.append({\n",
    "            'run_id': run_id,\n",
    "            'train_f1_score': train_f1_score,\n",
    "            'validation_f1_score': val_f1_score,\n",
    "            'test_f1_score': test_f1_score\n",
    "        })\n",
    "\n",
    "    # Convert the list of data into a pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Display or return the DataFrame\n",
    "    display(df)\n",
    "else:\n",
    "    print(f\"Experiment {experiment} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d65d1ff-a314-4174-b771-c61184645164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 3: Register the Best Model Using the UI\n",
    "\n",
    "1. If you have not already done so, navigate to **Experiments** on the left sidebar menu.\n",
    "1. Find the AutoML experiment you ran previously and click on the name. \n",
    "1. Select the name of the top run in the table. Notice the sort widget says `val_f1_score` automatically, so the table prioritizes this metric be default. \n",
    "1. Select **Register model** at the top right. \n",
    "1. In the dialog box, you will be presented with two options: Workspace Model Registry and Unity Catalog. If you select to register the model at the workspace-level, you will need to simply enter in the name of the model. If you wish to use Unity Catalog, you will need to provide the model name in the format `<catalog_name>.<schema_name>.<model_name>`. To keep our data assets manageable for the clean-up, let's select **Unity Catalog** and enter **`churn_prediction_base_model`** as model name for the catalog and schema we've been working in. For example, `firstName_lastName_7pig_da.default.churn_prediction_base_model` would be acceptable. \n",
    "1. Finally, click the **Register** button to complete the registration process. You will see a message indicating the registration process has started. \n",
    "1. Navigate to **Catalog**. After finding the catalog and schema where you saved your model, you will find your newly registered model either in the **Catalog Explorer** or by clicking on the schema and selecting the **Models** tab. \n",
    "\n",
    "Your model is now registered and ready for inferencing. We will discuss how to query a model in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947f8ba2-31cf-447f-87df-9dca8500e15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Clean-up\n",
    "\n",
    "After completing the demo, clean up any resources created.\n",
    "\n",
    "- Delete saved models from model registry.\n",
    "\n",
    "- Delete AutoML experiment from **Experiments** page.\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "261d4079-27ad-4af1-9f11-7a38f416d02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39dedf7e-298c-4c43-8933-3da748b1a2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conclusion And Next Steps\n",
    "\n",
    "In this demo, we learned how to utilize AutoML as a low-code solution for establishing a baseline champion model for a sample dataset. We learned about the various configurations that can be used within the UI. We also briefly explored how to approach AutoML from a programmatic point of view. Finally, we introduced the idea of registering your model for inferencing. In the next lesson, we will dig more into concepts such as packaging custom code, staging a model, and inferencing our registered models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e058cd34-664f-43fd-88eb-af1fdaee8f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02 - Experimentation with Mosaic AI AutoML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}