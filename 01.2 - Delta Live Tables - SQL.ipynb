{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a39a1e77-273a-4eeb-bf38-7ffeeae0ca24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data engineering with Databricks - Building our C360 database\n",
    "\n",
    "Building a C360 database requires to ingest multiple datasources.  \n",
    "\n",
    "It's a complex process requiring batch loads and streaming ingestion to support real-time insights, used for personalization and marketing targeting among other.\n",
    "\n",
    "Ingesting, transforming and cleaning data to create clean SQL tables for our downstream user (Data Analysts and Data Scientists) is complex.\n",
    "\n",
    "<link href=\"https://fonts.googleapis.com/css?family=DM Sans\" rel=\"stylesheet\"/>\n",
    "<div style=\"width:300px; text-align: center; float: right; margin: 30px 60px 10px 10px;  font-family: 'DM Sans'\">\n",
    "  <div style=\"height: 250px; width: 300px;  display: table-cell; vertical-align: middle; border-radius: 50%; border: 25px solid #fcba33ff;\">\n",
    "    <div style=\"font-size: 70px;  color: #70c4ab; font-weight: bold\">\n",
    "      73%\n",
    "    </div>\n",
    "    <div style=\"color: #1b5162;padding: 0px 30px 0px 30px;\">of enterprise data goes unused for analytics and decision making</div>\n",
    "  </div>\n",
    "  <div style=\"color: #bfbfbf; padding-top: 5px\">Source: Forrester</div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "## <img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/de.png\" style=\"float:left; margin: -35px 0px 0px 0px\" width=\"80px\"> John, as Data engineer, spends immense timeâ€¦.\n",
    "\n",
    "\n",
    "* Hand-coding data ingestion & transformations and dealing with technical challenges:<br>\n",
    "  *Supporting streaming and batch, handling concurrent operations, small files issues, GDPR requirements, complex DAG dependencies...*<br><br>\n",
    "* Building custom frameworks to enforce quality and tests<br><br>\n",
    "* Building and maintaining scalable infrastructure, with observability and monitoring<br><br>\n",
    "* Managing incompatible governance models from different systems\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "This results in **operational complexity** and overhead, requiring expert profile and ultimatly **putting data projects at risk**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7e60bf2-4071-40bb-b8e6-dfac5d1cfcbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simplify Ingestion and Transformation with Delta Live Tables\n",
    "\n",
    "<img style=\"float: right\" width=\"500px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/retail/lakehouse-churn/lakehouse-retail-c360-churn-1.png\" />\n",
    "\n",
    "In this notebook, we'll work as a Data Engineer to build our c360 database. <br>\n",
    "We'll consume and clean our raw data sources to prepare the tables required for our BI & ML workload.\n",
    "\n",
    "We have 3 data sources sending new files in our blob storage (`/demos/retail/churn/`) and we want to incrementally load this data into our Datawarehousing tables:\n",
    "\n",
    "- Customer profile data *(name, age, adress etc)*\n",
    "- Orders history *(what our customer bough over time)*\n",
    "- Streaming Events from our application *(when was the last time customers used the application, typically a stream from a Kafka queue)*\n",
    "\n",
    "\n",
    "Databricks simplify this task with Delta Live Table (DLT) by making Data Engineering accessible to all.\n",
    "\n",
    "DLT allows Data Analysts to create advanced pipeline with plain SQL.\n",
    "\n",
    "## Delta Live Table: A simple way to build and manage data pipelines for fresh, high quality data!\n",
    "\n",
    "<div>\n",
    "  <div style=\"width: 45%; float: left; margin-bottom: 10px; padding-right: 45px\">\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-accelerate.png\"/> \n",
    "      <strong>Accelerate ETL development</strong> <br/>\n",
    "      Enable analysts and data engineers to innovate rapidly with simple pipeline development and maintenance \n",
    "    </p>\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-complexity.png\"/> \n",
    "      <strong>Remove operational complexity</strong> <br/>\n",
    "      By automating complex administrative tasks and gaining broader visibility into pipeline operations\n",
    "    </p>\n",
    "  </div>\n",
    "  <div style=\"width: 48%; float: left\">\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-trust.png\"/> \n",
    "      <strong>Trust your data</strong> <br/>\n",
    "      With built-in quality controls and quality monitoring to ensure accurate and useful BI, Data Science, and ML \n",
    "    </p>\n",
    "    <p>\n",
    "      <img style=\"width: 50px; float: left; margin: 0px 5px 30px 0px;\" src=\"https://raw.githubusercontent.com/QuentinAmbard/databricks-demo/main/retail/resources/images/lakehouse-retail/logo-stream.png\"/> \n",
    "      <strong>Simplify batch and streaming</strong> <br/>\n",
    "      With self-optimization and auto-scaling data pipelines for batch or streaming processing \n",
    "    </p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<br style=\"clear:both\">\n",
    "\n",
    "<img src=\"https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-logo.png\" style=\"float: right;\" width=\"200px\">\n",
    "\n",
    "## Delta Lake\n",
    "\n",
    "All the tables we'll create in the Lakehouse will be stored as Delta Lake table. Delta Lake is an open storage framework for reliability and performance.<br>\n",
    "It provides many functionalities (ACID Transaction, DELETE/UPDATE/MERGE, Clone zero copy, Change data Capture...)<br>\n",
    "For more details on Delta Lake, run dbdemos.install('delta-lake')\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://www.google-analytics.com/collect?v=1&gtm=GTM-NKQ8TT7&tid=UA-163989034-1&cid=555&aip=1&t=event&ec=field_demos&ea=display&dp=%2F42_field_demos%2Fretail%2Flakehouse_churn%2Fdlt_sql&dt=LAKEHOUSE_RETAIL_CHURN\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c349502-2485-478e-9faf-b4784d055d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1/ Loading our data using Databricks Autoloader (cloud_files)\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-1.png\"/>\n",
    "</div>\n",
    "  \n",
    "Autoloader allow us to efficiently ingest millions of files from a cloud storage, and support efficient schema inference and evolution at scale.\n",
    "\n",
    "Let's use it to our pipeline and ingest the raw JSON & CSV data being delivered in our blob cloud storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaf61710-fe6a-4de8-8690-55659d6a6973",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw app events stream in incremental mode"
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING LIVE TABLE churn_app_events (\n",
    "  CONSTRAINT correct_schema EXPECT (_rescued_data IS NULL)\n",
    ")\n",
    "COMMENT \"Application events and sessions\"\n",
    "AS SELECT * FROM cloud_files(\"${rawDataVolumeLoc}/events\", \"csv\", map(\"cloudFiles.inferColumnTypes\", \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55f83153-052c-43d7-ab1b-91c594b2b205",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw orders from ERP"
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING LIVE TABLE churn_orders_bronze (\n",
    "  CONSTRAINT orders_correct_schema EXPECT (_rescued_data IS NULL)\n",
    ")\n",
    "COMMENT \"Spending score from raw data\"\n",
    "AS SELECT * FROM cloud_files(\"${rawDataVolumeLoc}/orders\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c149f8-dce4-4562-9cc7-59cd1d22e380",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw user data"
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING LIVE TABLE churn_users_bronze (\n",
    "  CONSTRAINT correct_schema EXPECT (_rescued_data IS NULL)\n",
    ")\n",
    "COMMENT \"raw user data coming from json files ingested in incremental with Auto Loader to support schema inference and evolution\"\n",
    "AS SELECT * FROM cloud_files(\"${rawDataVolumeLoc}/users\", \"json\", map(\"cloudFiles.inferColumnTypes\", \"true\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def79079-7572-4692-a768-9e1a2cd59461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2/ Enforce quality and materialize our tables for Data Analysts\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-2.png\"/>\n",
    "</div>\n",
    "\n",
    "The next layer often call silver is consuming **incremental** data from the bronze one, and cleaning up some information.\n",
    "\n",
    "We're also adding an [expectation](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html) on different field to enforce and track our Data Quality. This will ensure that our dashboard are relevant and easily spot potential errors due to data anomaly.\n",
    "\n",
    "These tables are clean and ready to be used by the BI team!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4b2eb4a-15d6-4b12-aba4-3760adb111be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and anonymise User data"
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING LIVE TABLE churn_users (\n",
    "  CONSTRAINT user_valid_id EXPECT (user_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "TBLPROPERTIES (pipelines.autoOptimize.zOrderCols = \"id\")\n",
    "COMMENT \"User data cleaned and anonymized for analysis.\"\n",
    "AS SELECT\n",
    "  id as user_id,\n",
    "  sha1(email) as email, \n",
    "  to_timestamp(creation_date, \"MM-dd-yyyy HH:mm:ss\") as creation_date, \n",
    "  to_timestamp(last_activity_date, \"MM-dd-yyyy HH:mm:ss\") as last_activity_date, \n",
    "  initcap(firstname) as firstname, \n",
    "  initcap(lastname) as lastname, \n",
    "  address, \n",
    "  channel, \n",
    "  country,\n",
    "  cast(gender as int),\n",
    "  cast(age_group as int), \n",
    "  cast(churn as int) as churn\n",
    "from STREAM(live.churn_users_bronze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f3189bf-b1d1-41fd-b504-55efa72b4d6b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean orders"
    }
   },
   "outputs": [],
   "source": [
    "CREATE STREAMING LIVE TABLE churn_orders (\n",
    "  CONSTRAINT order_valid_id EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW, \n",
    "  CONSTRAINT order_valid_user_id EXPECT (user_id IS NOT NULL) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"Order data cleaned and anonymized for analysis.\"\n",
    "AS SELECT\n",
    "  cast(amount as int),\n",
    "  id as order_id,\n",
    "  user_id,\n",
    "  cast(item_count as int),\n",
    "  to_timestamp(transaction_date, \"MM-dd-yyyy HH:mm:ss\") as creation_date\n",
    "\n",
    "from STREAM(live.churn_orders_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2acb03db-df43-4bc7-8ed4-c037fbab5ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3/ Aggregate and join data to create our ML features\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-3.png\"/>\n",
    "</div>\n",
    "\n",
    "We're now ready to create the features required for our Churn prediction.\n",
    "\n",
    "We need to enrich our user dataset with extra information which our model will use to help predicting churn, sucj as:\n",
    "\n",
    "* last command date\n",
    "* number of item bought\n",
    "* number of actions in our website\n",
    "* device used (ios/iphone)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0c174cc-8d94-49d5-970b-86cbf6810d7f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the feature table"
    }
   },
   "outputs": [],
   "source": [
    "CREATE LIVE TABLE churn_features\n",
    "COMMENT \"Final user table with all information for Analysis / ML\"\n",
    "AS \n",
    "  WITH \n",
    "    churn_orders_stats AS (SELECT user_id, count(*) as order_count, sum(amount) as total_amount, sum(item_count) as total_item, max(creation_date) as last_transaction\n",
    "      FROM live.churn_orders GROUP BY user_id),  \n",
    "    churn_app_events_stats as (\n",
    "      SELECT first(platform) as platform, user_id, count(*) as event_count, count(distinct session_id) as session_count, max(to_timestamp(date, \"MM-dd-yyyy HH:mm:ss\")) as last_event\n",
    "        FROM live.churn_app_events GROUP BY user_id)\n",
    "\n",
    "  SELECT *, \n",
    "         datediff(now(), creation_date) as days_since_creation,\n",
    "         datediff(now(), last_activity_date) as days_since_last_activity,\n",
    "         datediff(now(), last_event) as days_last_event\n",
    "       FROM live.churn_users\n",
    "         INNER JOIN churn_orders_stats using (user_id)\n",
    "         INNER JOIN churn_app_events_stats using (user_id)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01.2 - Delta Live Tables - SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
