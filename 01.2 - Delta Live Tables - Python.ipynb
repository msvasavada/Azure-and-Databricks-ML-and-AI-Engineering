{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77300113-fc97-46ca-9ffa-9038d9bcd52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The required imports that define the @dlt decorator\n",
    "import dlt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# The path to the blob storage with the raw data\n",
    "rawDataDirectory = \"/cloud_lakehouse_labs/retail/raw\"\n",
    "eventsRawDataDir = rawDataDirectory + \"/events\"\n",
    "ordersRawDataDir = rawDataDirectory + \"/orders\"\n",
    "usersRawDataDir = rawDataDirectory + \"/users\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c085a50b-28e5-48a0-b3d0-cab6b49ff9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1/ Loading our data using Databricks Autoloader (cloud_files)\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-1.png\"/>\n",
    "</div>\n",
    "  \n",
    "Autoloader allow us to efficiently ingest millions of files from a cloud storage, and support efficient schema inference and evolution at scale.\n",
    "\n",
    "Let's use it to our pipeline and ingest the raw JSON & CSV data being delivered in our blob cloud storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f06900cd-539a-4a9b-b2c5-a8f54caa5451",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw app events stream in incremental mode"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Application events and sessions\")\n",
    "@dlt.expect(\"App events correct schema\", \"_rescued_data IS NULL\")\n",
    "def churn_app_events():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(eventsRawDataDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14ba8b6c-1350-4ee0-ac1b-aff284b8198b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw orders from ERP"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Spending score from raw data\")\n",
    "@dlt.expect(\"Orders correct schema\", \"_rescued_data IS NULL\")\n",
    "def churn_orders_bronze():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(ordersRawDataDir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d867a320-6ff5-4823-a264-0af503d712e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest raw user data"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Raw user data coming from json files ingested in incremental with Auto Loader to support schema inference and evolution\")\n",
    "@dlt.expect(\"Users correct schema\", \"_rescued_data IS NULL\")\n",
    "def churn_users_bronze():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"json\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(usersRawDataDir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb1986d1-2c0d-44c5-abe4-8689d29d23ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2/ Enforce quality and materialize our tables for Data Analysts\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-2.png\"/>\n",
    "</div>\n",
    "\n",
    "The next layer often call silver is consuming **incremental** data from the bronze one, and cleaning up some information.\n",
    "\n",
    "We're also adding an [expectation](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html) on different field to enforce and track our Data Quality. This will ensure that our dashboard are relevant and easily spot potential errors due to data anomaly.\n",
    "\n",
    "These tables are clean and ready to be used by the BI team!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3493497e-814a-4140-8c93-99d4efb5e89a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean and anonymise User data"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"User data cleaned and anonymized for analysis.\")\n",
    "@dlt.expect_or_drop(\"user_valid_id\", \"user_id IS NOT NULL\")\n",
    "def churn_users():\n",
    "  return (dlt\n",
    "          .read_stream(\"churn_users_bronze\")\n",
    "          .select(F.col(\"id\").alias(\"user_id\"),\n",
    "                  F.sha1(F.col(\"email\")).alias(\"email\"), \n",
    "                  F.to_timestamp(F.col(\"creation_date\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"creation_date\"), \n",
    "                  F.to_timestamp(F.col(\"last_activity_date\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"last_activity_date\"), \n",
    "                  F.initcap(F.col(\"firstname\")).alias(\"firstname\"), \n",
    "                  F.initcap(F.col(\"lastname\")).alias(\"lastname\"), \n",
    "                  F.col(\"address\"), \n",
    "                  F.col(\"channel\"), \n",
    "                  F.col(\"country\"),\n",
    "                  F.col(\"gender\").cast(\"int\").alias(\"gender\"),\n",
    "                  F.col(\"age_group\").cast(\"int\").alias(\"age_group\"), \n",
    "                  F.col(\"churn\").cast(\"int\").alias(\"churn\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f896299-f227-42ed-bf21-332f0ab64e18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clean orders"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Order data cleaned and anonymized for analysis.\")\n",
    "@dlt.expect_or_drop(\"order_valid_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"order_valid_user_id\", \"user_id IS NOT NULL\")\n",
    "def churn_orders():\n",
    "  return (dlt\n",
    "          .read_stream(\"churn_orders_bronze\")\n",
    "          .select(F.col(\"amount\").cast(\"int\").alias(\"amount\"),\n",
    "                  F.col(\"id\").alias(\"order_id\"),\n",
    "                  F.col(\"user_id\"),\n",
    "                  F.col(\"item_count\").cast(\"int\").alias(\"item_count\"),\n",
    "                  F.to_timestamp(F.col(\"transaction_date\"), \"MM-dd-yyyy HH:mm:ss\").alias(\"creation_date\"))\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e7d6ad-68bd-43c6-ab65-92f25f6d019d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3/ Aggregate and join data to create our ML features\n",
    "<div style=\"float:right\">\n",
    "  <img width=\"500px\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/lakehouse-retail/lakehouse-retail-churn-de-small-3.png\"/>\n",
    "</div>\n",
    "\n",
    "We're now ready to create the features required for our Churn prediction.\n",
    "\n",
    "We need to enrich our user dataset with extra information which our model will use to help predicting churn, sucj as:\n",
    "\n",
    "* last command date\n",
    "* number of item bought\n",
    "* number of actions in our website\n",
    "* device used (ios/iphone)\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86005502-306a-4d26-9c7d-08e856d7cd02",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the feature table"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.create_table(comment=\"Final user table with all information for Analysis / ML\")\n",
    "def churn_features():\n",
    "  churn_app_events_stats_df = (dlt\n",
    "          .read(\"churn_app_events\")\n",
    "          .groupby(\"user_id\")\n",
    "          .agg(F.first(\"platform\").alias(\"platform\"),\n",
    "               F.count('*').alias(\"event_count\"),\n",
    "               F.count_distinct(\"session_id\").alias(\"session_count\"),\n",
    "               F.max(F.to_timestamp(\"date\", \"MM-dd-yyyy HH:mm:ss\")).alias(\"last_event\"))\n",
    "                              )\n",
    "  \n",
    "  churn_orders_stats_df = (dlt\n",
    "          .read(\"churn_orders\")\n",
    "          .groupby(\"user_id\")\n",
    "          .agg(F.count('*').alias(\"order_count\"),\n",
    "               F.sum(\"amount\").alias(\"total_amount\"),\n",
    "               F.sum(\"item_count\").alias(\"total_item\"),\n",
    "               F.max(\"creation_date\").alias(\"last_transaction\"))\n",
    "         )\n",
    "  \n",
    "  return (dlt\n",
    "          .read(\"churn_users\")\n",
    "          .join(churn_app_events_stats_df, on=\"user_id\")\n",
    "          .join(churn_orders_stats_df, on=\"user_id\")\n",
    "          .withColumn(\"days_since_creation\", F.datediff(F.current_timestamp(), F.col(\"creation_date\")))\n",
    "          .withColumn(\"days_since_last_activity\", F.datediff(F.current_timestamp(), F.col(\"last_activity_date\")))\n",
    "          .withColumn(\"days_last_event\", F.datediff(F.current_timestamp(), F.col(\"last_event\")))\n",
    "         )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01.2 - Delta Live Tables - Python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
