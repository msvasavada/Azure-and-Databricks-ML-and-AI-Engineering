{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68adef57-8194-4c67-bf95-38b4336379ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37339d93-a9af-4ea5-8c41-ef205fd47977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Assembling and Evaluating a RAG Application\n",
    "\n",
    "In the previous demo, we created a Vector Search Index. To build a complete RAG application, it is time to connect all the components that you have learned so far and evaluate the performance of the RAG.\n",
    "\n",
    "After evaluating the performance of the RAG pipeline, we will create and deploy a new Model Serving Endpoint to perform RAG.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "- Describe embeddings, vector databases, and search/retrieval as key components of implementing performant RAG applications.\n",
    "- Assemble a RAG pipeline by combining various components.\n",
    "- Build a RAG evaluation pipeline with MLflow evaluation functions.\n",
    "- Register a RAG pipeline to the Model Registry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e597e27-40d0-4d8d-bfe8-6d631a43959e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70ad03c7-f067-4bb1-b253-5bdc0235eb5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **15.4.x-cpu-ml-scala2.12**\n",
    "\n",
    "\n",
    "\n",
    "**\uD83D\uDEA8 Important: This demonstration relies on the resources established in the previous one. Please ensure you have completed the prior demonstration before starting this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d966e261-f821-4c9c-8f34-c2b939428126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c79724f-e6d9-4209-ab18-38544300f4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\njupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\nlangchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.63 which is incompatible.\nnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.11.7 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qq databricks-vectorsearch langchain==0.3.7 flashrank langchain-databricks PyPDF2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf78b8ff-ea4e-42bc-8faa-262f7a2c6bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66d71b7-86ae-4444-87d7-e577e7bf5812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de366d1-02f7-4a02-b645-f772d81b130f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb049e11-3dfe-4233-b7fa-a9f35a219703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser10822842_1751576779@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser10822842_1751576779\nWorking Directory: /Volumes/dbacademy/ops/labuser10822842_1751576779@vocareum_com\nDataset Location:  NestedNamespace (arxiv='/Volumes/dbacademy_arxiv/v01', dais='/Volumes/dbacademy_dais/v01', news='/Volumes/dbacademy_news/v01', docs='/Volumes/dbacademy_docs/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ca789e-84ca-4490-a388-9fdcc51319b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Demo Overview\n",
    "\n",
    "As seen in the diagram below, in this demo we will focus on the inference section (highlighted in green). The main focus of the previous demos was  Step 1 - Data preparation and vector storage. Now, it is time to put all components together to create a RAG application. \n",
    "\n",
    "The flow will be the following:\n",
    "\n",
    "- A user asks a question\n",
    "- The question is sent to our serverless Chatbot RAG endpoint\n",
    "- The endpoint compute the embeddings and searches for docs similar to the question, leveraging the Vector Search Index\n",
    "- The endpoint creates a prompt enriched with the doc\n",
    "- The prompt is sent to the Foundation Model Serving Endpoint\n",
    "- We display the output to our users!\n",
    "\n",
    "\n",
    "<!-- <img src=\"https://files.training.databricks.com/images/genai/genai-as-01-llm-rag-self-managed-flow-2.png\" width=\"100%\"> -->\n",
    "\n",
    "<!--  -->\n",
    "\n",
    "![genai-as-01-llm-rag-self-managed-flow-2](../Includes/images/genai-as-01-llm-rag-self-managed-flow-2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed9455ea-e8b7-4759-a1cc-a33ac625c780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup the RAG Components\n",
    "\n",
    "In this section, we will first define the components that we created before. Next, we will set up the retriever component for the application. Then, we will combine all the components together. In the final step, we will register the developed application as a model in the Model Registry with Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c74cbf5d-ed4d-4381-b191-cc9c8edede4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup the Retriever\n",
    "\n",
    "We will setup the Vector Search endpoint that we created in the previous demos as retriever. The retriever will return 2 relevant documents based on the query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e23c631-4b6d-4457-bc25-f41d8b873129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Vector Search endpoint name: vs_endpoint_2.\n"
     ]
    }
   ],
   "source": [
    "# components we created before\n",
    "# assign vs search endpoint by username\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "\n",
    "vs_endpoint_name = vs_endpoint_prefix + str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "print(f\"Assigned Vector Search endpoint name: {vs_endpoint_name}.\")\n",
    "\n",
    "vs_index_fullname = f\"{DA.catalog_name}.{DA.schema_name}.pdf_text_self_managed_vs_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de66efd6-24ad-4718-a402-36f7b4760a1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/6037/command-238634582065637-492335809:18: LangChainDeprecationWarning: Use databricks_langchain.DatabricksEmbeddings\n  embeddings = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant documents: [Document(metadata={'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf'}, page_content='There are 52 classes in R52, which consists of 70\\naverage tokens. It is divided into 6,532 and 2,568 training and testing texts.\\nTopic Labeling (TL) The task mainly obtains the meaning of the ﬁle by deﬁning complex ﬁle themes. It\\nis a critical component of topic analysis technology, which aims at simplifying topic analysis by assigning\\neach article to one or more topics. Here, we introduce a few in detail.\\nDBpedia [485] It is a large-scale multilingual knowledge base generated by Wikipedia’s most commonly\\nused information boxes. It releases DBpedia every month, adding or removing classes and attributes in each\\nversion. The most popular version of DBpedia has 14 categories, separated into 560,000 training data and\\n70,000 testing data. The number of average tokens is 55.\\nOhsumed [486] This is a biomedical literature database. The number of texts is 7,400. It has 23 car-\\ndiovascular disease categories and consists of 136 average tokens. All texts are medical abstracts that are\\ncategorized into one or more classes.\\nYahoo answers (YahooA) [423] The dataset is a topic labeling task having 10 categories. The number\\nof average tokens is 136. There are 140,000 training data and 5,000 testing data. Each text in YahooA has\\nquestion titles, question contexts, and best answers.\\nNatural Language Inference (NLI) This task is used to forecast whether the meaning of a text can be\\ninferred from another. Interpretation is a broad form of NLI. By comparing the semantic similarity of\\nsentence pairings, it determines whether a sentence is the interpretation of another one. Here we introduce\\nseveral primary datasets in detail.'), Document(metadata={'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf'}, page_content='RACE [492] The dataset has 5 categories, containing nearly 100,000 questions extracted from mid-\\ndle and high school English tests, with corresponding answers given by experts. The average length of\\nRACE text is more signiﬁcant than 300, which is longer than other reading comprehension datasets (such as\\nSQuAD) sequences.\\nDialog Act Classiﬁcation (DAC) The dialogue act is a speciﬁc verbal component, which marks the dia-\\nlogue according to the meaning category of the dialogue. DAC categorizes tags according to the meaning of\\nthe dialogue to help understand the speaker’s intentions.\\nDialog State Tracking Challenge 4 (DSTC 4) [450] It belongs to the dialog act classiﬁcation task and\\nmainly focuses on dialog state tracking on human-human dialogs. It is divided into 89 training classes and\\ncontains 24,000 training texts and 6,000 test texts.\\nICSI Meeting Recorder Dialog Act (MRDA) [451] It includes about 75 hours of speech from 75 naturally\\noccurring meetings among 53 speakers. The number of categories is 5, and it contains 51,000 training texts,\\n11,000 test texts, and 11,000 validation texts.\\nSwitchboard Dialog Act (SwDA) [493] The dataset extends the dialogue behavior label with rounds/discourses.\\nThe label summarizes the sentence structure, and relevant and pragmatic information of the relevant turn.\\nThe SwDA is split into 43 training classes and includes 1,003,000 training texts, 19,000 test texts, and\\n112,000 validation texts.\\n60\\nText Summarization Text summarization is a summary of given single or multiple documents. It is kept\\nas concise as possible while ensuring that it reﬂects the critical content of the original document.')]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-febd1d3618424192a0933cb155167020\"",
      "text/plain": [
       "Trace(request_id=tr-febd1d3618424192a0933cb155167020)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_databricks import DatabricksEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.docstore.document import Document\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "def get_retriever(cache_dir=\"/tmp\"):\n",
    "\n",
    "    def retrieve(query, k: int=10):\n",
    "        if isinstance(query, dict):\n",
    "            query = next(iter(query.values()))\n",
    "\n",
    "        # get the vector search index\n",
    "        vsc = VectorSearchClient(disable_notice=True)\n",
    "        vs_index = vsc.get_index(endpoint_name=vs_endpoint_name, index_name=vs_index_fullname)\n",
    "        \n",
    "        # get the query vector\n",
    "        embeddings = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "        query_vector = embeddings.embed_query(query)\n",
    "        \n",
    "        # get similar k documents\n",
    "        return query, vs_index.similarity_search(\n",
    "            query_vector=query_vector,\n",
    "            columns=[\"pdf_name\", \"content\"],\n",
    "            num_results=k)\n",
    "\n",
    "    def rerank(query, retrieved, cache_dir, k: int=2):\n",
    "        # format result to align with reranker lib format \n",
    "        passages = []\n",
    "        for doc in retrieved.get(\"result\", {}).get(\"data_array\", []):\n",
    "            new_doc = {\"file\": doc[0], \"text\": doc[1]}\n",
    "            passages.append(new_doc)       \n",
    "        # Load the flashrank ranker\n",
    "        ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=cache_dir)\n",
    "\n",
    "        # rerank the retrieved documents\n",
    "        rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "        results = ranker.rerank(rerankrequest)[:k]\n",
    "\n",
    "        # format the results of rerank to be ready for prompt\n",
    "        return [Document(page_content=r.get(\"text\"), metadata={\"source\": r.get(\"file\")}) for r in results]\n",
    "\n",
    "    # the retriever is a runnable sequence of retrieving and reranking.\n",
    "    return RunnableLambda(retrieve) | RunnableLambda(lambda x: rerank(x[0], x[1], cache_dir))\n",
    "\n",
    "# test our retriever\n",
    "question = {\"input\": \"How does Generative AI impact humans?\"}\n",
    "vectorstore = get_retriever(cache_dir = f\"{DA.paths.working_dir}/opt\")\n",
    "similar_documents = vectorstore.invoke(question)\n",
    "print(f\"Relevant documents: {similar_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b0e6d3-a655-4d62-b6e6-1640e0eb46e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup the Foundation Model\n",
    "\n",
    "Our chatbot will be using `llama3.3` foundation model to provide answer. \n",
    "\n",
    "While the model is available using the built-in [Foundation endpoint](/ml/endpoints), we can use Databricks Langchain Chat Model wrapper to easily build our chain.  \n",
    "\n",
    "Note: multiple type of endpoint or langchain models can be used.\n",
    "\n",
    "- Databricks Foundation models (what we'll use)\n",
    "- Your fined-tune model\n",
    "- An external model provider (such as Azure OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581c6c30-f553-4eb1-b382-35ffa3c3e12a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/6037/command-238634582065639-842269141:4: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  chat_model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 300)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chat model: content='Generative AI refers to a type of artificial intelligence that is capable of generating new, original content, such as images, videos, music, text, or even entire datasets. This is in contrast to traditional AI, which is typically focused on analyzing and processing existing data.\\n\\nGenerative AI uses complex algorithms and neural networks to learn patterns and structures from large datasets, and then generates new content based on that learning. The goal of generative AI is to create new, synthetic data that is similar in style, structure, and quality to the original data.\\n\\nThere are several types of generative AI, including:\\n\\n1. **Generative Adversarial Networks (GANs)**: GANs consist of two neural networks that work together to generate new content. One network generates new data, while the other network evaluates the generated data and tells the first network whether it is realistic or not.\\n2. **Variational Autoencoders (VAEs)**: VAEs are a type of neural network that learns to compress and reconstruct data. They can be used to generate new data by sampling from the compressed representation.\\n3. **Recurrent Neural Networks (RNNs)**: RNNs are a type of neural network that is well-suited for generating sequential data, such as text or music.\\n4. **Transformers**: Transformers are a type of neural network that is particularly well-suited for generating text and other sequential data.\\n\\nGenerative AI has many potential applications, including:\\n\\n1. **' additional_kwargs={} response_metadata={'prompt_tokens': 16, 'completion_tokens': 300, 'total_tokens': 316} id='run--18c77738-bf5e-4198-82fb-3088b1c5e3a3-0'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-a7d212101b864108a79080b332245a35\"",
      "text/plain": [
       "Trace(request_id=tr-a7d212101b864108a79080b332245a35)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "# test Databricks Foundation LLM model\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 300)\n",
    "print(f\"Test chat model: {chat_model.invoke('What is Generative AI?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1fe4d48-fd36-444f-bae4-ee6c5976918e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Assembling the Complete RAG Solution\n",
    "\n",
    "Let's now merge the retriever and the model in a single Langchain chain.\n",
    "\n",
    "We will use a custom langchain template for our assistant to give proper answer.\n",
    "\n",
    "Make sure you take some time to try different templates and adjust your assistant tone and personality for your requirement.\n",
    "\n",
    "<!-- <img src=\"https://files.training.databricks.com/images/genai/genai-as-01-llm-rag-self-managed-model-2.png\" width=\"100%\" /> -->\n",
    "\n",
    "![genai-as-01-llm-rag-self-managed-model-2](../Includes/images/genai-as-01-llm-rag-self-managed-model-2.png)\n",
    "\n",
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33a48f75-0078-4e50-b93d-25ee1216662e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Some important notes about the LangChain formatting:\n",
    "\n",
    "* Context documents retrieved from the vector store are added by separated newline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac31761-b072-4bde-a5af-97353473da37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "TEMPLATE = \"\"\"You are an assistant for GENAI teaching class. You are answering questions related to Generative AI and how it impacts humans life. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "Use the following pieces of context to answer the question at the end:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"input\"])\n",
    "\n",
    "# unwrap the longchain document from the context to be a dict so we can register the signature in mlflow\n",
    "def unwrap_document(answer):\n",
    "  return answer | {\"context\": [{\"metadata\": r.metadata, \"page_content\": r.page_content} for r in answer[\"context\"]]}\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(chat_model, prompt)\n",
    "chain = create_retrieval_chain(get_retriever(), question_answer_chain)|RunnableLambda(unwrap_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc0715b-0b40-47ab-8e41-7c277885c361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rrank-T5-flan.zip:   0%|          | 0.00/73.7M [00:00<?, ?iB/s]\rrank-T5-flan.zip:  16%|█▋        | 12.0M/73.7M [00:00<00:00, 125MiB/s]\rrank-T5-flan.zip:  46%|████▌     | 33.9M/73.7M [00:00<00:00, 186MiB/s]\rrank-T5-flan.zip:  83%|████████▎ | 61.0M/73.7M [00:00<00:00, 231MiB/s]\rrank-T5-flan.zip: 100%|██████████| 73.7M/73.7M [00:00<00:00, 221MiB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'How does Generative AI impact humans?', 'context': [{'metadata': {'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf'}, 'page_content': 'There are 52 classes in R52, which consists of 70\\naverage tokens. It is divided into 6,532 and 2,568 training and testing texts.\\nTopic Labeling (TL) The task mainly obtains the meaning of the ﬁle by deﬁning complex ﬁle themes. It\\nis a critical component of topic analysis technology, which aims at simplifying topic analysis by assigning\\neach article to one or more topics. Here, we introduce a few in detail.\\nDBpedia [485] It is a large-scale multilingual knowledge base generated by Wikipedia’s most commonly\\nused information boxes. It releases DBpedia every month, adding or removing classes and attributes in each\\nversion. The most popular version of DBpedia has 14 categories, separated into 560,000 training data and\\n70,000 testing data. The number of average tokens is 55.\\nOhsumed [486] This is a biomedical literature database. The number of texts is 7,400. It has 23 car-\\ndiovascular disease categories and consists of 136 average tokens. All texts are medical abstracts that are\\ncategorized into one or more classes.\\nYahoo answers (YahooA) [423] The dataset is a topic labeling task having 10 categories. The number\\nof average tokens is 136. There are 140,000 training data and 5,000 testing data. Each text in YahooA has\\nquestion titles, question contexts, and best answers.\\nNatural Language Inference (NLI) This task is used to forecast whether the meaning of a text can be\\ninferred from another. Interpretation is a broad form of NLI. By comparing the semantic similarity of\\nsentence pairings, it determines whether a sentence is the interpretation of another one. Here we introduce\\nseveral primary datasets in detail.'}, {'metadata': {'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2302.09419.pdf'}, 'page_content': 'RACE [492] The dataset has 5 categories, containing nearly 100,000 questions extracted from mid-\\ndle and high school English tests, with corresponding answers given by experts. The average length of\\nRACE text is more signiﬁcant than 300, which is longer than other reading comprehension datasets (such as\\nSQuAD) sequences.\\nDialog Act Classiﬁcation (DAC) The dialogue act is a speciﬁc verbal component, which marks the dia-\\nlogue according to the meaning category of the dialogue. DAC categorizes tags according to the meaning of\\nthe dialogue to help understand the speaker’s intentions.\\nDialog State Tracking Challenge 4 (DSTC 4) [450] It belongs to the dialog act classiﬁcation task and\\nmainly focuses on dialog state tracking on human-human dialogs. It is divided into 89 training classes and\\ncontains 24,000 training texts and 6,000 test texts.\\nICSI Meeting Recorder Dialog Act (MRDA) [451] It includes about 75 hours of speech from 75 naturally\\noccurring meetings among 53 speakers. The number of categories is 5, and it contains 51,000 training texts,\\n11,000 test texts, and 11,000 validation texts.\\nSwitchboard Dialog Act (SwDA) [493] The dataset extends the dialogue behavior label with rounds/discourses.\\nThe label summarizes the sentence structure, and relevant and pragmatic information of the relevant turn.\\nThe SwDA is split into 43 training classes and includes 1,003,000 training texts, 19,000 test texts, and\\n112,000 validation texts.\\n60\\nText Summarization Text summarization is a summary of given single or multiple documents. It is kept\\nas concise as possible while ensuring that it reﬂects the critical content of the original document.'}], 'answer': \"Generative AI has the potential to significantly impact humans in various aspects of life, including automation of tasks, improvement of decision-making, and enhancement of creativity. However, the context provided does not give a direct answer to this question. It mainly discusses various datasets and tasks related to natural language processing, such as topic labeling, natural language inference, and text summarization, without explicitly stating how Generative AI impacts humans. Therefore, I don't know the answer based on the given context.\"}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-df5703ce7ba349c59a4fc101d1ffd9c8\"",
      "text/plain": [
       "Trace(request_id=tr-df5703ce7ba349c59a4fc101d1ffd9c8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = {\"input\": \"How does Generative AI impact humans?\"}\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c65578fe-5998-443b-bd1b-83aaa7863483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save the Model to Model Registry in UC\n",
    "\n",
    "Now that our model is ready and evaluated, we can register it within our Unity Catalog schema. \n",
    "\n",
    "After registering the model, you can view the model and models in the **Catalog Explorer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fc52b4d-9fcc-4be7-8dd9-9f3c8f1653b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-92fc5404-f124-41fc-b24a-99cc046a5740/lib/python3.11/site-packages/langchain/chains/api/base.py:56: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n\n  from langchain_community.utilities.requests import TextRequestsWrapper\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-92fc5404-f124-41fc-b24a-99cc046a5740/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `TextRequestsWrapper` to V2.\n  warn(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-92fc5404-f124-41fc-b24a-99cc046a5740/lib/python3.11/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n* 'allow_population_by_field_name' has been renamed to 'validate_by_name'\n  warnings.warn(message, UserWarning)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-92fc5404-f124-41fc-b24a-99cc046a5740/lib/python3.11/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n* 'underscore_attrs_are_private' has been removed\n  warnings.warn(message, UserWarning)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-92fc5404-f124-41fc-b24a-99cc046a5740/lib/python3.11/site-packages/mlflow/langchain/runnables.py:292: UserWarning: Your model contains a class imported from the LangChain partner package `langchain-databricks`. When loading the model back, MLflow will use the community version of the classes instead of the partner packages, which may lead to unexpected behavior. To ensure that the model is loaded correctly, it is recommended to save the model with the 'model-from-code' method instead: https://mlflow.org/docs/latest/models.html#models-from-code\n  warnings.warn(\n2025/07/03 22:00:23 INFO mlflow: Attempting to auto-detect Databricks resource dependencies for the current langchain model. Dependency auto-detection is best-effort and may not capture all dependencies of your langchain model, resulting in authorization errors when serving or querying your model. We recommend that you explicitly pass `resources` to mlflow.langchain.log_model() to ensure authorization to dependent resources succeeds when the model is deployed.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46421cc61414dbab4ad265c99ae3461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'dbacademy.labuser10822842_1751576779.rag_app_demo4'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d8f54c6da04c27a54a681030aaef4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'dbacademy.labuser10822842_1751576779.rag_app_demo4'.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "# set model registry to UC\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.rag_app_demo4\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"rag_app_demo4\") as run:\n",
    "    signature = infer_signature(question, answer)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever, \n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow.__version__,\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        input_example=question,\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "944de8ab-cc62-4fa4-b31f-93eb9f6d7c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Resources\n",
    "\n",
    "This is the final demo. You can delete all resources created in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f717d8b-dd1d-4a23-a660-25fac578acff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we illustrated the process of constructing a comprehensive RAG application utilizing a variety of Databricks products. Initially, we established the RAG components that were previously created in the earlier demos, namely the Vector Search endpoint and Vector Search index. Subsequently, we constructed the retriever component and set up the foundational model for use. Following this, we put together the entire RAG application and evaluated the performance of the pipeline using MLflow's LLM evaluation functions. As a final step, we registered the newly created RAG application as a model within the Model Registry with Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9749aae8-78dd-495e-a92f-43f2fec55b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.1 - Assembling and Evaluating a RAG Application",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}